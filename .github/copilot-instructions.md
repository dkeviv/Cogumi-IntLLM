r

**# GITHUB COPILOT MASTER INSTRUCTIONS - QINS CHAT DEMO

Project: IntLLM - Projective Integer Large Language Model (Chat Demo Edition)
Version: 1.1
Purpose: Complete implementation guide for interactive Gradio chat with Phi-3.5 on CPU/M4

---

## PROJECT OVERVIEW

### What We're Building

Interactive chat demo showing QINS compression + inference on consumer hardware:

* Model: Phi-3.5-mini-instruct (3.8B parameters)
* Core Innovation: Logarithmic encoding with inverse magnitude mapping (large |w| â†’ stored=1)
* Memory: FP32 ~7.6 GB â†’ QINS ~1.9 GB (4Ã— reduction)
* Interface: Gradio web-based chat (streaming responses)
* Target Hardware: M4 MacBook (24GB RAM) - CPU/MPS inference
* Features: Multi-turn conversation, real-time stats, token streaming

### Why Phi-3.5-mini?

âœ… Excellent instruction following
âœ… 128K context window (great for chat)
âœ… 3.8B params = perfect size for demo
âœ… FP32 barely works on 8GB systems (good "wow" factor)
âœ… QINS version runs smoothly on CPU

---

## CRITICAL COPILOT PRINCIPLES

### What Copilot Does Well

âœ… Complete functions from clear docstrings
âœ… Implement standard algorithms (sampling, softmax)
âœ… Write Gradio interface components
âœ… Add type hints and error handling
âœ… Generate boilerplate code

### What Copilot Struggles With

âŒ Novel mathematical concepts (projective transformation)
âŒ Streaming generation loops
âŒ M4/MPS device optimization
âŒ Multi-turn conversation formatting
âŒ Complex Gradio layouts

### How to Guide Copilot Successfully

Write signature + docstring FIRST:

python

defgenerate_streaming(self, prompt: str, max_tokens: int=512) -> Iterator[str]:

    """

    Generate response with token-by-token streaming.

    Algorithm:

    1. Tokenize prompt to input_ids

    2. Loop for max_tokens iterations:

    a. Forward pass: logits = model(input_ids).logits

    b. Get last token logits: logits[:, -1, :]

    c. Apply temperature: logits = logits / temperature

    d. Sample: next_token = torch.multinomial(probs, 1)

    e. Decode: token_text = tokenizer.decode(next_token)

    f. Yield token_text for streaming display

    g. Append to input_ids: torch.cat([input_ids, next_token])

    h. Break if EOS token

    Yields:

    Individual token strings for real-time display

    """

    # Copilot will complete this correctly

Use TODO comments with specific libraries:

python

# TODO: Use torch.multinomial() for sampling, not argmax

# TODO: Apply softmax before multinomial: F.softmax(logits, dim=-1)

# TODO: Check device with torch.backends.mps.is_available()

# TODO: Use gr.ChatInterface() not gr.Chatbot() for multi-turn

```


---


## FILE STRUCTURE

```

projective-llm/

â”œâ”€â”€ CHANGELOG.md

â”œâ”€â”€ TECHNICAL_SPEC.md

â”œâ”€â”€ README.md

â”‚

â”œâ”€â”€ src/

â”‚   â”œâ”€â”€ __init__.py

â”‚   â”œâ”€â”€ projective_layer.py      # Core ProjectiveLinear layer

â”‚   â”œâ”€â”€ converter.py              # FP32 â†’ QINS conversion

â”‚   â”œâ”€â”€ compression.py            # Weight compression (sparsity + Huffman)

â”‚   â””â”€â”€ model_loader.py          # NEW: Load compressed QINS models

â”‚

â”œâ”€â”€ examples/

â”‚   â”œâ”€â”€ convert_phi35.py         # Convert Phi-3.5 to QINS

â”‚   â”œâ”€â”€ demo_chat.py             # NEW: Interactive Gradio chat

â”‚   â””â”€â”€ benchmark_memory.py      # NEW: Memory comparison demo

â”‚

â”œâ”€â”€ tests/

â”‚   â”œâ”€â”€ test_layer.py

â”‚   â”œâ”€â”€ test_conversion.py

â”‚   â”œâ”€â”€ test_chat.py             # NEW: Chat system tests

â”‚   â””â”€â”€ test_generation.py       # NEW: Generation quality tests

â”‚

â”œâ”€â”€ requirements.txt

â””â”€â”€ .gitignore

---

## CORE MATHEMATICAL FOUNDATION

```python
"""
CRITICAL: QINS uses LOGARITHMIC encoding with INVERSE magnitude relationship

STORAGE FORMAT (per weight):
  - stored: uint8 [1, 255] - magnitude encoding (INVERSE: large |w| â†’ small stored)
  - sign: int8 {-1, +1} - sign preservation (NEVER changes)
  - log_min, log_max: float32 per layer - reconstruction parameters

INVERSE MAGNITUDE RELATIONSHIP:
  - Large magnitude â†’ stored = 1 (small stored value)
  - Medium magnitude â†’ stored = 128 
  - Small magnitude â†’ stored = 255 (large stored value)

SIGN PRESERVATION:
  - Signs stored SEPARATELY in sign tensor
  - Original sign = stored sign = reconstructed sign (100% preserved)
  - Negative weight: sign = -1
  - Positive weight: sign = +1

CONVERSION (FP32 â†’ QINS Logarithmic):
  1. Extract signs: sign = torch.sign(weight)
  2. Take log of absolute weights: log_weight = log(|w|)
  3. Find log range: log_min, log_max
  4. Normalize: normalized = (log_weight - log_min) / (log_max - log_min)
  5. INVERSE map to [1, 255]: stored = 255 - (normalized * 254)
  6. Return: (stored, sign, log_min, log_max)

RECONSTRUCTION (QINS â†’ FP32):
  1. Reverse inverse mapping: normalized = (255 - stored) / 254
  2. Map to log space: log_weight = log_min + normalized * (log_max - log_min)
  3. Exponentiate: magnitude = exp(log_weight)
  4. Apply sign: w = sign Ã— magnitude
  
EXAMPLE:
  Original: -0.490955
  â†’ Magnitude: 0.490955 (largest in layer)
  â†’ stored = 1 (inverse: large magnitude â†’ small stored)
  â†’ sign = -1 (preserved)
  â†’ Reconstructed: -0.490955 (sign Ã— decoded_magnitude)
  
MEMORY:
  - FP32: 4 bytes per weight
  - QINS: 2 bytes per weight (1 byte stored + 1 byte sign)
  - Compression: 2.00Ã— (will be ~4Ã— with bit-packed signs)

QUALITY:
  - Mean relative error: <1% (typically 0.8-1.5%)
  - Sign preservation: 100%
  - Inverse relationship: Verified
"""
```

---

## IMPLEMENTATION GUIDE

### Phase 1: Model Loader (NEW - Required for Chat)

File:src/model_loader.py

python

"""

FILE: model_loader.py

PURPOSE: Load compressed QINS models for inference

DEPENDENCIES: torch, transformers, compression.py

CRITICAL CONCEPTS:

- Load compressed weights from disk
- Decompress through pipeline (reverse of compression)
- Reconstruct QINS model structure
- Move to appropriate device (CPU/MPS)

WORKFLOW:

1. Load compressed file
2. Decompress weights
3. Load model architecture
4. Replace Linear layers with ProjectiveLinear
5. Load decompressed weights into model
6. Move to device

"""

import torch

import torch.nn as nn

from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig

from pathlib import Path

import pickle

from typing import Tuple, Optional

from .projective_layer import ProjectiveLinear

from .compression import ProjectiveCompressor

from .converter import convert_model_to_projective

classQINSModelLoader:

    """

    Load and prepare QINS models for inference.

    Handles:

    - Decompression of stored weights

    - Model architecture reconstruction

    - Device placement (CPU/MPS/CUDA)

    - Memory-efficient loading

    Usage:

    loader = QINSModelLoader()

    model, tokenizer = loader.load("models/phi35-qins-compressed.bin")

    model.eval()

    """

    def__init__(self, device: Optional[str] =None):

    """

    Initialize loader.

    Args:

    device: Target device. If None, auto-detect (MPS > CUDA > CPU)

    """

    if device isNone:

    device = self._detect_device()

    self.device = device

    self.compressor = ProjectiveCompressor()

    print(f"QINSModelLoader initialized on device: {device}")

    def_detect_device(self) ->str:

    """

    Auto-detect best available device.

    Priority: MPS (M4) > CUDA > CPU

    Returns:

    Device string: "mps", "cuda", or "cpu"

    Algorithm:

    1. Check torch.backends.mps.is_available() for M4

    2. Check torch.cuda.is_available() for NVIDIA

    3. Fallback to CPU

    """

    # TODO: Implement device detection

    # HINT: if torch.backends.mps.is_available():

    # HINT:     return "mps"

    # HINT: elif torch.cuda.is_available():

    # HINT:     return "cuda"

    # HINT: else:

    # HINT:     return "cpu"

    pass

    defload(

    self,

    compressed_path: str,

    model_name: str="microsoft/Phi-3.5-mini-instruct"

    ) -> Tuple[nn.Module, AutoTokenizer]:

    """

    Load compressed QINS model.

    Args:

    compressed_path: Path to .compressed file

    model_name: HuggingFace model ID (for architecture)

    Returns:

    (model, tokenizer) tuple ready for inference

    Algorithm:

    1. Load compressed file from disk

    2. Decompress weights using ProjectiveCompressor

    3. Load base model architecture (empty weights)

    4. Convert to ProjectiveLinear layers

    5. Load decompressed QINS weights

    6. Move to device

    7. Load tokenizer

    8. Return (model, tokenizer)

    Example:

    model, tokenizer = loader.load(

    "models/phi35-qins.compressed",

    "microsoft/Phi-3.5-mini-instruct"

    )

    """

    print(f"Loading compressed model from {compressed_path}...")

    # TODO: Load compressed file

    # HINT: with open(compressed_path, 'rb') as f:

    # HINT:     compressed_data = pickle.load(f)

    # TODO: Decompress weights

    # HINT: weights_dict = self.compressor.decompress(compressed_data)

    print(f"Decompressed {len(weights_dict)} weight tensors")

    # TODO: Load model architecture (empty)

    # HINT: config = AutoConfig.from_pretrained(model_name)

    # HINT: model = AutoModelForCausalLM.from_config(config)

    # NOTE: This loads architecture only, not pretrained weights

    # TODO: Convert to QINS

    # HINT: from .converter import convert_model_to_projective

    # HINT: model = convert_model_to_projective(model, verbose=False)

    # TODO: Load decompressed weights into model

    # HINT: model.load_state_dict(weights_dict, strict=False)

    # NOTE: strict=False because we have ProjectiveLinear not nn.Linear

    # TODO: Move to devicef

    # HINT: model = model.to(self.device)

    # HINT: model.eval()  # Set to inference mode

    # TODO: Load tokenizer

    # HINT: tokenizer = AutoTokenizer.from_pretrained(model_name)

    # HINT: if tokenizer.pad_token is None:

    # HINT:     tokenizer.pad_token = tokenizer.eos_token

    print(f"âœ“ Model loaded on {self.device}")

    return model, tokenizer

    defload_from_pretrained(

    self,

    model_name: str="microsoft/Phi-3.5-mini-instruct"

    ) -> Tuple[nn.Module, AutoTokenizer]:

    """

    Load and convert model from HuggingFace (no compression).

    Use this for quick testing without pre-compressed weights.

    Will download full FP32 model then convert to QINS.

    Args:

    model_name: HuggingFace model ID

    Returns:

    (model, tokenizer) in QINS format

    WARNING: This loads FP32 first (uses ~8GB), then converts.

    For production, use pre-compressed weights with load().

    """

    print(f"Loading {model_name} from HuggingFace...")

    print("WARNING: This downloads FP32 weights (~7.6 GB)")

    # TODO: Load FP32 model

    # HINT: model = AutoModelForCausalLM.from_pretrained(

    # HINT:     model_name,

    # HINT:     torch_dtype=torch.float32,

    # HINT:     trust_remote_code=True

    # HINT: )

    # TODO: Convert to QINS

    # HINT: model = convert_model_to_projective(model)

    # TODO: Move to device

    # HINT: model = model.to(self.device)

    # HINT: model.eval()

    # TODO: Load tokenizer

    # HINT: tokenizer = AutoTokenizer.from_pretrained(model_name)

    return model, tokenizer

defsave_compressed_model(

    model: nn.Module,

    save_path: str,

    compressor: Optional[ProjectiveCompressor] =None

):

    """

    Save QINS model in compressed format.

    Args:

    model: QINS model (with ProjectiveLinear layers)

    save_path: Where to save .compressed file

    compressor: Compression instance (creates if None)

    Algorithm:

    1. Extract all ProjectiveLinear weight tensors

    2. Create weights_dict {layer_name: stored_weights}

    3. Compress using ProjectiveCompressor

    4. Save to disk with pickle

    Example:

    save_compressed_model(

    qins_model,

    "models/phi35-qins.compressed"

    )

    """

    if compressor isNone:

    compressor = ProjectiveCompressor()

    print(f"Saving compressed model to {save_path}...")

    # TODO: Extract QINS weights

    # weights_dict = {}

    # for name, module in model.named_modules():

    #     if isinstance(module, ProjectiveLinear):

    #         weights_dict[f"{name}.stored"] = module.stored

    #         weights_dict[f"{name}.sign"] = module.sign

    # TODO: Compress

    # compressed_data = compressor.compress(weights_dict)

    # TODO: Save

    # with open(save_path, 'wb') as f:

    #     pickle.dump(compressed_data, f)

    print(f"âœ“ Saved compressed model ({len(compressed_data)} bytes)")

---

### Phase 2: Chat System (NEW - Main Demo)

File:examples/demo_chat.py

python

"""

FILE: demo_chat.py

PURPOSE: Interactive Gradio chat interface for QINS Phi-3.5

DEPENDENCIES: gradio, torch, transformers, psutil

CRITICAL CONCEPTS:

- Multi-turn conversation with history formatting
- Token-by-token streaming for real-time display
- Memory monitoring (show QINS advantage)
- M4-optimized inference (MPS device)
- No HuggingFace .generate() - custom loop

CHAT TEMPLATE (Phi-3.5):

<|system|>

You are a helpful AI assistant.<|end|>

<|user|>

User message here<|end|>

<|assistant|>

Assistant response here<|end|>

GENERATION ALGORITHM:

1. Format conversation with chat template
2. Tokenize to input_ids
3. Loop until EOS or max_tokens:

   - Forward pass through model
   - Sample next token (temperature + top-p)
   - Decode and yield token
   - Append to input_ids
4. Return full response

"""

import gradio as gr

import torch

import torch.nn.functional as F

import psutil

import os

from typing import Iterator, List, Tuple

import time

from pathlib import Path

# Add src to path for imports

import sys

sys.path.insert(0, str(Path(__file__).parent.parent))

from src.model_loader import QINSModelLoader

classQINSChatSystem:

    """

    Interactive chat system for QINS models.

    Features:

    - Multi-turn conversation management

    - Streaming token generation (ChatGPT-like)

    - Memory usage monitoring

    - Temperature and top-p sampling

    - M4/MPS optimization

    Usage:

    chat = QINSChatSystem("models/phi35-qins.compressed")

    for token in chat.generate_streaming("Hello!", []):

    print(token, end='', flush=True)

    """

    def__init__(

    self,

    model_path: str,

    device: str=None,

    load_from_hub: bool=False

    ):

    """

    Initialize chat system.

    Args:

    model_path: Path to compressed model OR HuggingFace model ID

    device: "cpu", "mps", "cuda", or None (auto-detect)

    load_from_hub: If True, download from HuggingFace

    """

    print("="*60)

    print("ğŸš€ Initializing QINS Chat System")

    print("="*60)

    start_time = time.time()

    mem_before = self._get_memory_gb()

    # Load model

    loader = QINSModelLoader(device=device)

    if load_from_hub:

    self.model, self.tokenizer = loader.load_from_pretrained(model_path)

    else:

    self.model, self.tokenizer = loader.load(

    model_path,

    "microsoft/Phi-3.5-mini-instruct"

    )

    self.device = loader.device

    # Load stats

    load_time = time.time() - start_time

    mem_after = self._get_memory_gb()

    self.model_memory = mem_after - mem_before

    print(f"âœ“ Loaded in {load_time:.1f}s")

    print(f"âœ“ Model memory: {self.model_memory:.2f} GB")

    print(f"âœ“ Device: {self.device}")

    print("="*60)

    # Generation settings (can be adjusted)

    self.max_new_tokens =512

    self.temperature =0.7

    self.top_p =0.9

    self.system_prompt ="You are a helpful AI assistant."

    def_get_memory_gb(self) ->float:

    """Get current process memory in GB."""

    # TODO: Use psutil to get memory

    # HINT: process = psutil.Process(os.getpid())

    # HINT: mem_bytes = process.memory_info().rss

    # HINT: return mem_bytes / (1024 ** 3)

    pass

    defformat_chat_history(

    self,

    message: str,

    history: List[Tuple[str, str]]

    ) ->str:

    """

    Format conversation for Phi-3.5 chat template.

    Args:

    message: Current user message

    history: List of (user_msg, assistant_msg) tuples

    Returns:

    Formatted prompt string with special tokens

    Phi-3.5 Format:

    <|system|>

    {system_prompt}<|end|>

    <|user|>

    {user_message}<|end|>

    <|assistant|>

    {assistant_response}<|end|>

    ...

    <|user|>

    {current_message}<|end|>

    <|assistant|>

    Algorithm:

    1. Start with system prompt

    2. Add each turn from history

    3. Add current message

    4. End with assistant tag (model continues here)

    """

    # TODO: Build prompt string

    # prompt = f"<|system|>\n{self.system_prompt}<|end|>\n"

    #

    # for user_msg, assistant_msg in history:

    #     prompt += f"<|user|>\n{user_msg}<|end|>\n"

    #     prompt += f"<|assistant|>\n{assistant_msg}<|end|>\n"

    #

    # prompt += f"<|user|>\n{message}<|end|>\n"

    # prompt += "<|assistant|>\n"

    #

    # return prompt

    pass

    defgenerate_streaming(

    self,

    message: str,

    history: List[Tuple[str, str]],

    max_new_tokens: int=None,

    temperature: float=None,

    top_p: float=None

    ) -> Iterator[str]:

    """

    Generate response with token streaming.

    Args:

    message: User's message

    history: Conversation history

    max_new_tokens: Override default max tokens

    temperature: Override default temperature

    top_p: Override default top-p

    Yields:

    Token strings for streaming display

    Algorithm:

    1. Format prompt with history

    2. Tokenize: input_ids = tokenizer(prompt)

    3. Move to device

    4. Generation loop:

    a. Forward pass: logits = model(input_ids).logits

    b. Get last position: logits = logits[:, -1, :]

    c. Apply temperature: logits = logits / temperature

    d. Top-p filtering (nucleus sampling)

    e. Sample: next_token = multinomial(softmax(logits))

    f. Check EOS: break if next_token == eos_token_id

    g. Decode: token_text = tokenizer.decode(next_token)

    h. Yield token_text

    i. Append: input_ids = cat([input_ids, next_token])

    5. Done when EOS or max_tokens reached

    CRITICAL: Use torch.no_grad() to save memory

    CRITICAL: Use torch.multinomial() not argmax (for diversity)

    """

    # Use defaults if not specified

    max_new_tokens = max_new_tokens or self.max_new_tokens

    temperature = temperature or self.temperature

    top_p = top_p or self.top_p

    # TODO: Format prompt

    # prompt = self.format_chat_history(message, history)

    # TODO: Tokenize

    # input_ids = self.tokenizer(prompt, return_tensors="pt").input_ids

    # input_ids = input_ids.to(self.device)

    # TODO: Generation loop

    # accumulated_text = ""

    #

    # with torch.no_grad():

    #     for _ in range(max_new_tokens):

    #         # Forward pass

    #         outputs = self.model(input_ids)

    #         logits = outputs.logits[:, -1, :]  # Shape: (1, vocab_size)

    #

    #         # Temperature scaling

    #         logits = logits / temperature

    #

    #         # Top-p (nucleus) sampling

    #         sorted_logits, sorted_indices = torch.sort(logits, descending=True)

    #         cumulative_probs = torch.cumsum(

    #             F.softmax(sorted_logits, dim=-1),

    #             dim=-1

    #         )

    #

    #         # Remove tokens with cumulative prob > top_p

    #         sorted_indices_to_remove = cumulative_probs > top_p

    #         sorted_indices_to_remove[..., 0] = False  # Keep at least one

    #

    #         indices_to_remove = sorted_indices[sorted_indices_to_remove]

    #         logits[0, indices_to_remove] = float('-inf')

    #

    #         # Sample from filtered distribution

    #         probs = F.softmax(logits, dim=-1)

    #         next_token = torch.multinomial(probs, num_samples=1)

    #

    #         # Check for end of sequence

    #         if next_token.item() == self.tokenizer.eos_token_id:

    #             break

    #

    #         # Decode token

    #         token_text = self.tokenizer.decode(next_token[0], skip_special_tokens=True)

    #

    #         # Yield for streaming

    #         accumulated_text += token_text

    #         yield accumulated_text

    #

    #         # Append for next iteration

    #         input_ids = torch.cat([input_ids, next_token], dim=-1)

    pass

    defchat(

    self,

    message: str,

    history: List[Tuple[str, str]]

    ) -> Iterator[str]:

    """

    Main chat function called by Gradio.

    This is the entry point for the Gradio ChatInterface.

    Args:

    message: User's message

    history: Gradio chat history format

    Yields:

    Accumulated response text for streaming

    """

    yieldfrom self.generate_streaming(message, history)

    defget_stats(self) ->dict:

    """

    Get current system statistics.

    Returns:

    Dictionary with memory, device, model info

    """

    return {

    "memory_gb": self._get_memory_gb(),

    "model_memory_gb": self.model_memory,

    "device": self.device,

    "max_tokens": self.max_new_tokens,

    "temperature": self.temperature,

    "top_p": self.top_p

    }

defcreate_gradio_interface(chat_system: QINSChatSystem) -> gr.Blocks:

    """

    Create Gradio web interface.

    Layout:

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

    â”‚  ğŸš€ QINS Chat - Phi-3.5 on CPU             â”‚

    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

    â”‚                         â”‚  ğŸ“Š Stats        â”‚

    â”‚  Chat Interface         â”‚                  â”‚

    â”‚  (streaming)            â”‚  Memory: 1.9 GB  â”‚

    â”‚                         â”‚  Device: mps     â”‚

    â”‚  [User input box]       â”‚  Temp: 0.7       â”‚

    â”‚  [Send button]          â”‚  Top-p: 0.9      â”‚

    â”‚                         â”‚                  â”‚

    â”‚  [Clear] [Examples]     â”‚  [Refresh]       â”‚

    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Args:

    chat_system: Initialized QINSChatSystem

    Returns:

    Gradio Blocks interface

    """

    with gr.Blocks(

    title="QINS Chat Demo",

    theme=gr.themes.Soft()

    ) as demo:

    # Header

    gr.Markdown("""

    # ğŸš€ QINS Chat - Phi-3.5-mini on CPU

    **Quantum Integer Numerical System** - 4Ã— compression, full quality

    ---

    **Model:** Phi-3.5-mini-instruct (3.8B parameters)

    **Memory:** ~1.9 GB (vs ~7.6 GB FP32)

    **Compression:** 4Ã— through inverse magnitude encoding

    **Device:** CPU/MPS (M4 optimized)

    """)

    with gr.Row():

    # Left column: Chat interface

    with gr.Column(scale=3):

    chatbot = gr.Chatbot(

    label="Chat History",

    height=500,

    type="messages"

    )

    with gr.Row():

    msg = gr.Textbox(

    label="Your message",

    placeholder="Type your message here...",

    lines=2,

    scale=4

    )

    send_btn = gr.Button("Send", scale=1, variant="primary")

    with gr.Row():

    clear_btn = gr.Button("ğŸ—‘ï¸ Clear Chat")

    # Example prompts

    gr.Examples(

    examples=[

    "Explain quantum computing in simple terms",

    "Write a haiku about artificial intelligence",

    "What is the QINS compression method?",

    "Compare Python and Rust for systems programming"

    ],

    inputs=msg,

    label="ğŸ’¡ Example Prompts"

    )

    # Right column: Stats and controls

    with gr.Column(scale=1):

    gr.Markdown("### ğŸ“Š System Stats")

    stats = chat_system.get_stats()

    memory_display = gr.Textbox(

    label="Current Memory",

    value=f"{stats['memory_gb']:.2f} GB",

    interactive=False

    )

    gr.Markdown(f"""

    **Model Memory:** {stats['model_memory_gb']:.2f} GB

    **Device:** {stats['device']}

    **FP32 equivalent:** ~7.6 GB

    **Compression:** 4Ã—

    ---

    ### âš™ï¸ Generation Settings

    """)

    temperature_slider = gr.Slider(

    minimum=0.1,

    maximum=2.0,

    value=stats['temperature'],

    step=0.1,

    label="Temperature",

    info="Higher = more creative"

    )

    top_p_slider = gr.Slider(

    minimum=0.1,

    maximum=1.0,

    value=stats['top_p'],

    step=0.05,

    label="Top-p",

    info="Nucleus sampling threshold"

    )

    max_tokens_slider = gr.Slider(

    minimum=50,

    maximum=2048,

    value=stats['max_tokens'],

    step=50,

    label="Max Tokens",

    info="Maximum response length"

    )

    defupdate_stats():

    stats = chat_system.get_stats()

    returnf"{stats['memory_gb']:.2f} GB"

    refresh_btn = gr.Button("ğŸ”„ Refresh Stats")

    refresh_btn.click(

    fn=update_stats,

    outputs=memory_display

    )

    # Footer info

    gr.Markdown("""

    ---

    ### ğŸ’¡ About QINS

    **Quantum Integer Numerical System** uses logarithmic encoding with inverse magnitude mapping:

    - Stored value 1 â†’ highest magnitude (large weights)

    - Stored value 128 â†’ medium magnitude

    - Stored value 255 â†’ lowest magnitude (small weights)

    - Signs stored separately and preserved exactly (100%)

    **Encoding:** `log(|w|) â†’ normalize â†’ inverse map to [1,255]`

    This enables:

    - âœ… 4Ã— memory reduction (INT8 vs FP32)

    - âœ… Faster CPU inference

    - âœ… <1% accuracy loss

    - âœ… Natural precision allocation (more bits for critical small weights)

    - âœ… Perfect sign preservation

    """)

    # Chat interaction logic

    defrespond(message, chat_history, temperature, top_p, max_tokens):

    """Handle chat interaction with streaming."""

    # Update settings

    chat_system.temperature = temperature

    chat_system.top_p = top_p

    chat_system.max_new_tokens = max_tokens

    # Generate response

    bot_message =""

    for partial_response in chat_system.generate_streaming(message, chat_history):

    bot_message = partial_response

    yield chat_history + [[message, bot_message]]

    # TODO: Wire up interactions

    # HINT: msg.submit() handles Enter key

    # HINT: send_btn.click() handles button click

    # HINT: Both should call respond() function

    #

    # msg.submit(

    #     fn=respond,

    #     inputs=[msg, chatbot, temperature_slider, top_p_slider, max_tokens_slider],

    #     outputs=chatbot

    # ).then(

    #     fn=lambda: "",  # Clear input box

    #     outputs=msg

    # )

    #

    # send_btn.click(

    #     fn=respond,

    #     inputs=[msg, chatbot, temperature_slider, top_p_slider, max_tokens_slider],

    #     outputs=chatbot

    # ).then(

    #     fn=lambda: "",

    #     outputs=msg

    # )

    #

    # clear_btn.click(

    #     fn=lambda: None,

    #     outputs=chatbot

    # )

    return demo

defmain():

    """

    Launch QINS chat demo.

    Usage:

    python examples/demo_chat.py [--model MODEL_PATH] [--device DEVICE] [--hub]

    Arguments:

    --model: Path to compressed model or HuggingFace model ID

    --device: cpu/mps/cuda (auto-detect if not specified)

    --hub: Load from HuggingFace instead of compressed file

    Example:

    # From compressed file (recommended)

    python examples/demo_chat.py --model models/phi35-qins.compressed

    # From HuggingFace (downloads FP32 then converts)

    python examples/demo_chat.py --model microsoft/Phi-3.5-mini-instruct --hub

    """

    import argparse

    parser = argparse.ArgumentParser(description="QINS Chat Demo")

    parser.add_argument(

    "--model",

    type=str,

    default="models/phi35-qins.compressed",

    help="Path to compressed model or HuggingFace ID"

    )

    parser.add_argument(

    "--device",

    type=str,

    default=None,

    help="Device: cpu/mps/cuda (auto if not specified)"

    )

    parser.add_argument(

    "--hub",

    action="store_true",

    help="Load from HuggingFace Hub"

    )

    parser.add_argument(

    "--port",

    type=int,

    default=7860,

    help="Port for Gradio interface"

    )

    parser.add_argument(

    "--share",

    action="store_true",

    help="Create public link"

    )

    args = parser.parse_args()

    # Initialize chat system

    chat_system = QINSChatSystem(

    model_path=args.model,

    device=args.device,

    load_from_hub=args.hub

    )

    # Create and launch interface

    demo = create_gradio_interface(chat_system)

    print("\n"+"="*60)

    print("ğŸŒ Launching Gradio interface...")

    print("="*60)

    demo.launch(

    server_name="0.0.0.0",

    server_port=args.port,

    share=args.share

    )

if __name__ =="__main__":

    main()

---

### Phase 3: Conversion Script (Phi-3.5 specific) - Logarithmic Encoding

File:examples/convert_phi35.py

python

"""

FILE: convert_phi35.py

PURPOSE: Convert Phi-3.5-mini to QINS and compress

DEPENDENCIES: transformers, torch, src modules

WORKFLOW:

1. Download Phi-3.5-mini from HuggingFace (~7.6 GB FP32)
2. Convert all Linear layers to ProjectiveLinear
3. Verify conversion accuracy
4. Compress with sparsity + Huffman
5. Save compressed model (~1.9 GB)

USAGE:

    python examples/convert_phi35.py --output models/phi35-qins.compressed

"""

"""

QINS Conversion & Reconstruction

Complete implementation with all imports

"""

import torch

import torch.nn.functional as F

def convert_to_qins(weight: torch.Tensor):

    """

    Convert FP32 weights to QINS INT8 format (logarithmic encoding).

    Preserves inverse relationship: large weights â†’ small stored values

    Args:

    weight: FP32 weight tensor of any shape

    Returns:

    stored: uint8 tensor [1, 255] - inverse magnitude

    sign: int8 tensor {-1, +1} - signs

    log_min: float - min log weight (for reconstruction)

    log_max: float - max log weight (for reconstruction)

    Algorithm:

    1. Extract signs separately

    2. Take log of absolute weights

    3. Map log range to [1, 255] with INVERSE relationship:

    - Large weights (large log) â†’ small stored (near 1)

    - Small weights (small log) â†’ large stored (near 255)

    """

    # Extract signs

    sign = torch.sign(weight).to(torch.int8)

    sign[sign == 0] = 1  # Handle exact zeros as positive

    # Get absolute weights, clamp to avoid log(0)

    abs_weight = torch.abs(weight).clamp(min=1e-8)

    # Log space transformation

    log_weight = torch.log(abs_weight)

    # Find log range (only from non-zero weights)

    non_zero_mask = torch.abs(weight) > 1e-8

    if non_zero_mask.sum() == 0:

    # All zeros edge case

    stored = torch.ones_like(weight, dtype=torch.uint8) * 128

    return stored, sign, torch.tensor(-10.0), torch.tensor(0.0)

    log_min = log_weight[non_zero_mask].min()

    log_max = log_weight[non_zero_mask].max()

    # Normalize to [0, 1]

    # Large weights â†’ normalized near 1.0

    # Small weights â†’ normalized near 0.0

    normalized = (log_weight - log_min) / (log_max - log_min + 1e-8)

    # INVERSE mapping to [1, 255]

    # Large weights (normalized=1.0) â†’ stored=1

    # Small weights (normalized=0.0) â†’ stored=255

    stored_float = 255 - (normalized * 254)

    stored = stored_float.round().clamp(1, 255).to(torch.uint8)

    return stored, sign, log_min.item(), log_max.item()

def reconstruct_from_qins(stored, sign, log_min, log_max):

    """

    Reconstruct FP32 weights from QINS INT8 format.

    Args:

    stored: uint8 tensor [1, 255] - inverse magnitudes

    sign: int8 tensor {-1, +1} - signs

    log_min: float - min log weight

    log_max: float - max log weight

    Returns:

    weight: FP32 tensor - reconstructed weights

    Algorithm:

    1. Reverse inverse mapping: stored â†’ normalized

    2. Map normalized to log space

    3. Exponentiate to get absolute weights

    4. Apply signs

    """

    # Reverse inverse mapping

    # stored=1 â†’ normalized=1.0 (large weight)

    # stored=255 â†’ normalized=0.0 (small weight)

    normalized = (255.0 - stored.float()) / 254.0

    # Map back to log space

    log_weight = log_min + normalized * (log_max - log_min)

    # Exponentiate to get absolute weights

    abs_weight = torch.exp(log_weight)

    # Apply signs

    weight = sign.float() * abs_weight

    return weight

# ============================================================================

# NOTE: Old linear inverse mapping removed - use logarithmic encoding above

# ============================================================================

# ============================================================================

# USAGE EXAMPLE

# ============================================================================

if __name__ == "__main__":

    # Test conversion

    print("=" * 70)

    print("QINS Conversion Test")

    print("=" * 70)

    original_weight = torch.randn(100, 50) * 0.1  # Typical NN weights

    print("\nğŸ“Š Original weight stats:")

    print(f"  Min: {original_weight.min():.6f}")

    print(f"  Max: {original_weight.max():.6f}")

    print(f"  Mean: {original_weight.mean():.6f}")

    print(f"  Std: {original_weight.std():.6f}")

    # Convert

    print("\nğŸ”„ Converting to QINS...")

    stored, sign, log_min, log_max = convert_to_qins(original_weight)

    print(f"\nğŸ“¦ Stored value stats:")

    print(f"  Min: {stored.min()}")

    print(f"  Max: {stored.max()}")

    print(f"  Mean: {stored.float().mean():.1f}")

    print(f"  Unique values: {stored.unique().numel()}")

    print(f"  Log range: [{log_min:.4f}, {log_max:.4f}]")

    # Check inverse relationship

    print(f"\nâœ… Verifying inverse relationship:")

    max_weight_idx = original_weight.abs().argmax()

    min_weight_idx = original_weight.abs().argmin()

    print(f"  Largest weight ({original_weight.flatten()[max_weight_idx]:.6f}) â†’ stored = {stored.flatten()[max_weight_idx]}")

    print(f"  Smallest weight ({original_weight.flatten()[min_weight_idx]:.6f}) â†’ stored = {stored.flatten()[min_weight_idx]}")

    # Reconstruct

    print("\nğŸ”„ Reconstructing from QINS...")

    reconstructed_weight = reconstruct_from_qins(stored, sign, log_min, log_max)

    print(f"\nğŸ“Š Reconstructed weight stats:")

    print(f"  Min: {reconstructed_weight.min():.6f}")

    print(f"  Max: {reconstructed_weight.max():.6f}")

    print(f"  Mean: {reconstructed_weight.mean():.6f}")

    print(f"  Std: {reconstructed_weight.std():.6f}")

    # Error analysis

    abs_error = (original_weight - reconstructed_weight).abs()

    rel_error = abs_error / (original_weight.abs() + 1e-8)

    print(f"\nğŸ“‰ Conversion error:")

    print(f"  Mean absolute error: {abs_error.mean():.6f}")

    print(f"  Max absolute error: {abs_error.max():.6f}")

    print(f"  Mean relative error: {rel_error.mean():.4%}")

    print(f"  Max relative error: {rel_error.max():.4%}")

    # Memory savings

    fp32_bytes = original_weight.numel() * 4

    qins_bytes = stored.numel() * 1 + sign.numel() * 1 + 8  # +8 for log_min/log_max

    print(f"\nğŸ’¾ Memory usage:")

    print(f"  FP32: {fp32_bytes:,} bytes")

    print(f"  QINS: {qins_bytes:,} bytes")

    print(f"  Compression: {fp32_bytes/qins_bytes:.2f}Ã—")

    print("\n" + "=" * 70)

    print("âœ“ Test complete!")

    print("=" * 70)

    # Step 3: Verify accuracy

    print("\nâœ… Verifying conversion accuracy...")

    print("(Testing first Linear layer)")

    # TODO: Add verification code

    # Find first Linear layer in original and converted

    # Measure error with measure_conversion_error()

    # Print results

    # Step 4: Compress

    ifnot args.skip_compression:

    print("\nğŸ—œï¸  Compressing weights...")

    compressor = ProjectiveCompressor(phase=1)  # Phase 1: sparsity + Huffman

    save_compressed_model(model, args.output, compressor)

    print(f"âœ“ Saved to {args.output}")

    else:

    print("\nğŸ’¾ Saving uncompressed QINS model...")

    torch.save(model.state_dict(), args.output)

    print(f"âœ“ Saved to {args.output}")

    print("\n"+"="*60)

    print("âœ“ CONVERSION COMPLETE")

    print("="*60)

    print(f"\nNext steps:")

    print(f"1. Test inference:")

    print(f"   python examples/demo_chat.py --model {args.output}")

    print(f"2. Or load in Python:")

    print(f"   from src.model_loader import QINSModelLoader")

    print(f"   loader = QINSModelLoader()")

    print(f"   model, tokenizer = loader.load('{args.output}')")

if __name__ =="__main__":

    main()

```


---


## DEPENDENCIES


**File:** `requirements.txt`

```

# Core dependencies

torch>=2.0.0

transformers>=4.36.0

accelerate>=0.25.0

# Chat interface

gradio>=4.0.0

# Compression

numpy>=1.24.0

# System monitoring

psutil>=5.9.0

# Development

pytest>=7.4.0

black>=23.0.0

# Optional: For M4 optimization

# (torch with MPS support included in torch>=2.0)

---

## TESTING

File:tests/test_chat.py

python

"""

FILE: test_chat.py

PURPOSE: Test chat system functionality

"""

import pytest

import torch

from examples.demo_chat import QINSChatSystem

@pytest.fixture

defmock_chat_system():

    """Create chat system with small test model."""

    # TODO: Create minimal test model or mock

    pass

deftest_chat_history_formatting():

    """Test Phi-3.5 chat template formatting."""

    # TODO: Verify chat template format

    pass

deftest_streaming_generation():

    """Test token streaming works."""

    # TODO: Verify streaming yields tokens

    pass

deftest_temperature_sampling():

    """Test temperature affects diversity."""

    # TODO: Generate with different temperatures

    pass

deftest_top_p_filtering():

    """Test top-p nucleus sampling."""

    # TODO: Verify top-p filters tokens correctly

    pass

---

## USAGE INSTRUCTIONS

### Quick Start (After Implementation)

bash

# 1. Install dependencies

pip install -r requirements.txt

# 2. Convert Phi-3.5 to QINS

python examples/convert_phi35.py --output models/phi35-qins.compressed

# 3. Launch chat interface

python examples/demo_chat.py --model models/phi35-qins.compressed

# 4. Open browser to http://localhost:7860

### For M4 Optimization

bash

# Use MPS device explicitly

python examples/demo_chat.py --model models/phi35-qins.compressed --device mps

### For Quick Testing (Skip Compression)

bash

# Load directly from HuggingFace (slower, uses more memory)

python examples/demo_chat.py \

    --model microsoft/Phi-3.5-mini-instruct \

    --hub \

    --device mps

---

## COMMON PITFALLS

### 1. Chat Template Formatting

âŒ Wrong: Missing special tokens

python

prompt =f"User: {message}\nAssistant: "

âœ… Correct: Use Phi-3.5 format

python

prompt =f"<|user|>\n{message}<|end|>\n<|assistant|>\n"

### 2. Sampling vs Argmax

âŒ Wrong: Always picking most likely token (deterministic)

python

next_token = logits.argmax()

âœ… Correct: Sample for diversity

python

probs = F.softmax(logits / temperature, dim=-1)

next_token = torch.multinomial(probs, 1)

### 3. Memory Management

âŒ Wrong: Keeping gradients

python

for _ inrange(max_tokens):

    outputs = model(input_ids)  # Builds computation graph!

âœ… Correct: Use no_grad

python

with torch.no_grad():

    for _ inrange(max_tokens):

    outputs = model(input_ids)

### 4. Device Mismatch

âŒ Wrong: Model on CPU, input on MPS

python

input_ids = tokenizer(text, return_tensors="pt").input_ids  # CPU

output = model(input_ids)  # Model on MPS â†’ error

âœ… Correct: Move input to model's device

python

input_ids = tokenizer(text, return_tensors="pt").input_ids

input_ids = input_ids.to(model.device)

output = model(input_ids)

### 5. Gradio Streaming

âŒ Wrong: Returning full string at end

python

defchat(message, history):

    response = generate_all(message)

    return response  # No streaming!

âœ… Correct: Yield incrementally

python

defchat(message, history):

    accumulated =""

    for token in generate_streaming(message):

    accumulated += token

    yield accumulated  # Updates in real-time

---

## SUCCESS METRICS

### Chat Demo Complete When:

* Model loads in <10 seconds on M4
* Memory usage ~1.9 GB (vs ~7.6 GB FP32)
* Chat interface displays with stats
* Token streaming works (ChatGPT-like display)
* Multi-turn conversations maintain context
* Temperature and top-p sliders affect output
* Generation speed >3 tokens/sec on M4 CPU
* No crashes or memory leaks during extended chat
* Responses are coherent and on-topic

### Quality Metrics:

* Response quality: Similar to FP32 Phi-3.5
* Accuracy loss: <1% on benchmarks
* First token latency: <2 seconds
* Sustained throughput: 5-8 tokens/sec on M4

---

## NEXT STEPS

After completing chat demo:

1. Benchmarking (examples/benchmark_memory.py)
   * Side-by-side FP32 vs QINS comparison
   * Memory graphs over time
   * Speed measurements
2. Compression Optimization (Phase 2)

* Add RLE encoding
* Add dictionary compression
* Target 20-25Ã— compression

1. Mobile Deployment

* Export to Core ML for iOS
* Optimize for iPhone/iPad
* <1 GB final model size

---

END OF GITHUB COPILOT INSTRUCTIONS - CHAT DEMO EDITION

This file provides complete guidance for implementing an interactive Gradio chat interface with QINS-compressed Phi-3.5-mini. Follow the structure, implement TODOs with Copilot assistance, and test incrementally.

**
