#!/usr/bin/env python3
"""
Quick test: Load QINS model and generate a response
"""

import torch
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer
import sys
import os

sys.path.insert(0, 'src')
from projective_layer import ProjectiveLinear
import torch.nn as nn

print("="*70)
print("QINS Model Response Test")
print("="*70)

# Setup device
device = "mps" if torch.backends.mps.is_available() else "cpu"
print(f"\n‚úÖ Device: {device}")

# Load tokenizer
print("\nüì• Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3.5-mini-instruct")
tokenizer.pad_token = tokenizer.eos_token

# Load FP32 model
print("\nüì• Loading FP32 model...")
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-3.5-mini-instruct",
    torch_dtype=torch.float32,
    trust_remote_code=True
)

# Convert to QINS
print("\nüîÑ Converting to QINS...")
count = 0
for name, module in model.named_modules():
    for child_name, child in list(module.named_children()):
        if isinstance(child, nn.Linear):
            proj = ProjectiveLinear(
                child.in_features,
                child.out_features,
                bias=(child.bias is not None)
            )
            proj.from_linear(child)
            setattr(module, child_name, proj)
            count += 1
            if count % 20 == 0:
                print(f"  Converted {count} layers...", end='\r')

print(f"‚úÖ Converted {count} layers to QINS                    ")

# Move to device
print(f"\nüì§ Moving model to {device}...")
model = model.to(device)
model.eval()
print("‚úÖ Model ready!")

# Test generation
print("\n" + "="*70)
print("Testing Generation")
print("="*70)

prompt = "The future of artificial intelligence is"
print(f"\nüìù Prompt: \"{prompt}\"")
print(f"\nü§ñ Generating response...\n")

inputs = tokenizer(prompt, return_tensors="pt").to(device)
input_ids = inputs['input_ids']

# Generate tokens one by one
generated_text = prompt
max_tokens = 30

with torch.no_grad():
    for i in range(max_tokens):
        # Forward pass (disable cache to avoid compatibility issues)
        outputs = model(input_ids, use_cache=False)
        logits = outputs.logits[:, -1, :]
        
        # Sample next token
        probs = F.softmax(logits / 0.7, dim=-1)
        next_token = torch.multinomial(probs, 1)
        
        # Decode and print
        token_text = tokenizer.decode(next_token[0], skip_special_tokens=True)
        print(token_text, end='', flush=True)
        generated_text += token_text
        
        # Append for next iteration
        input_ids = torch.cat([input_ids, next_token], dim=-1)
        
        # Stop if EOS
        if next_token.item() == tokenizer.eos_token_id:
            break

print("\n\n" + "="*70)
print("‚úÖ Test Complete!")
print("="*70)
print(f"\nFull response:\n{generated_text}")
print("\n‚úÖ QINS model is working correctly!")
print("‚úÖ Weight caching is active (fast inference)")
print("‚úÖ Ready for full benchmark")
